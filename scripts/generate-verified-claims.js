#!/usr/bin/env node

/**
 * Interactive helper for verifying paper claims against all gathered evidence.
 *
 * Usage:
 *   node scripts/generate-verified-claims.js
 */

const fs = require("fs");
const path = require("path");
const readline = require("readline");
const clipboardModule = require("clipboardy");
const clipboardy = clipboardModule?.default ?? clipboardModule;

const DEFAULT_OUTPUT_PATH = path.join(__dirname, "../lib/mock-similar-papers.ts");

const CLEANUP_PROMPT_HEADER = `You are a cleanup agent. Convert the analyst's claim verification notes into strict JSON for Evidentia's verified claims UI.

Output requirements:
- Return a single JSON object with keys: claims (array), overallAssessment (string), promptNotes (optional string).
- Each claim object must include: claimId (string matching C1, C2, etc.), originalClaim (string), verificationStatus (string), supportingEvidence (array), contradictingEvidence (array), verificationSummary (string), confidenceLevel (string).
- verificationStatus must be one of: "Verified", "Partially Verified", "Contradicted", "Insufficient Evidence".
- confidenceLevel must be one of: "High", "Moderate", "Low".
- Each evidence item (supporting or contradicting) must have: source (string: "Similar Paper", "Patent", "Research Group", or "Thesis"), title (string), relevance (string explaining the connection).
- verificationSummary must be a detailed 2-3 sentence explanation of the verification status and reasoning.
- overallAssessment should be a brief paragraph summarizing the paper's overall claim validity across all claims.
- No markdown, commentary, or trailing prose. Valid JSON only (double quotes).
- Preserve factual content from the notes; do not invent evidence.
- Output raw JSON only — no markdown fences, comments, trailing prose, or extra keys.`;

const CURLY_QUOTES_TO_ASCII = [
  [/\u2018|\u2019|\u201A|\u201B/g, "'"],
  [/\u201C|\u201D|\u201E|\u201F/g, '"'],
  [/\u2013|\u2014|\u2015|\u2212/g, "-"],
  [/\u2026/g, "..."],
  [/\u00A0/g, " "],
  [/\u200B|\u200C|\u200D|\uFEFF/g, ""],
  [/\u0000|\u0001|\u0002|\u0003|\u0004|\u0005|\u0006|\u0007|\u0008|\u0009|\u000A|\u000B|\u000C|\u000D/g, " "]
];

function cleanPlainText(input) {
  if (typeof input !== "string") {
    return input;
  }

  let value = input.replace(/\r\n/g, "\n").trim();
  for (const [pattern, replacement] of CURLY_QUOTES_TO_ASCII) {
    value = value.replace(pattern, replacement);
  }

  value = value.replace(/\[([^\]]+)\]\((https?:\/\/[^)]+)\)/g, (match, label, url) => `${label} (${url})`);
  value = value.replace(/\[(\d+|[a-zA-Z]+)\]/g, " $1");

  value = value
    .split("\n")
    .map((line) => line.trim())
    .filter((line, index, lines) => line.length > 0 || (index > 0 && lines[index - 1].length > 0))
    .join("\n");

  return value.trim();
}

function ensureDirExists(targetPath) {
  const dir = path.dirname(targetPath);
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
  }
}

function writeMockLibrary(outputPath, libraryData) {
  ensureDirExists(outputPath);
  const banner = `// Auto-generated by scripts/${path.basename(__filename)} on ${new Date().toISOString()}\n`;
  const warning = "// Do not edit by hand. Re-run the script with updated inputs.";
  const fileContents = `${banner}${warning}\n\nexport const MOCK_SIMILAR_PAPERS_LIBRARY = ${JSON.stringify(libraryData, null, 2)} as const;\n`;
  fs.writeFileSync(outputPath, fileContents, "utf-8");
}

function readExistingLibrary(outputPath) {
  if (!fs.existsSync(outputPath)) {
    return null;
  }

  try {
    const fileContents = fs.readFileSync(outputPath, "utf-8");
    const match = fileContents.match(/export const MOCK_SIMILAR_PAPERS_LIBRARY = (\{[\s\S]*\}) as const;/);
    if (!match || !match[1]) {
      return null;
    }
    return JSON.parse(match[1]);
  } catch (error) {
    console.warn("Failed to read existing mock library", error);
    return null;
  }
}

function createInterface() {
  return readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });
}

function ask(rl, question) {
  return new Promise((resolve) => {
    rl.question(question, (answer) => {
      resolve(answer);
    });
  });
}

async function collectCleanedJson(rl) {
  console.log("\nPaste the cleaned JSON now. Press ENTER on an empty line when you're done.");
  console.log("Press ENTER immediately to skip when you don't have output yet.\n");

  const lines = [];
  while (true) {
    const line = await ask(rl, "> ");
    const trimmed = line.trim();
    if (lines.length === 0 && !trimmed) {
      return "";
    }
    if (!trimmed) {
      break;
    }
    if (trimmed.toUpperCase() === "END") {
      break;
    }
    lines.push(line);
  }

  return lines.join("\n").trim();
}

function buildVerificationPrompt(library) {
  const claimsAnalysis = library?.claimsAnalysis;
  const similarPapers = Array.isArray(library?.similarPapers) ? library.similarPapers : [];
  const researchGroups = library?.researchGroups?.structured?.papers || [];
  const researcherTheses = library?.researcherTheses?.structured?.researchers || [];
  const patents = library?.patents?.structured?.patents || [];

  if (!claimsAnalysis || !claimsAnalysis.structured || !Array.isArray(claimsAnalysis.structured.claims)) {
    throw new Error("No claims analysis found. Run the claims analysis script first.");
  }

  const claims = claimsAnalysis.structured.claims;
  const paperTitle = library?.sourcePaper?.title || library?.sourcePdf?.title || "Unknown paper";
  const paperDoi = library?.sourcePdf?.doi || library?.sourcePaper?.doi || "";

  if (claims.length === 0) {
    throw new Error("No claims found in the analysis. Cannot generate verification prompt.");
  }

  const lines = [
    "You are a scientific claim verification analyst.",
    "",
    "Verify Paper Claims Against All Available Evidence",
    "",
    `Paper: ${paperTitle}`,
  ];

  if (paperDoi) {
    lines.push(`DOI: ${paperDoi}`);
  }

  lines.push(
    "",
    "Task: Cross-reference each claim below against ALL available evidence from similar papers, research groups, PhD theses, and patents. Determine verification status, identify supporting and contradicting evidence, and assess confidence level.",
    "",
    "=== CLAIMS TO VERIFY ===",
    ""
  );

  claims.forEach((claim) => {
    lines.push(`${claim.id}: ${claim.claim}`);
    if (claim.evidenceSummary) {
      lines.push(`   Evidence: ${claim.evidenceSummary}`);
    }
    if (claim.strength) {
      lines.push(`   Original Strength: ${claim.strength}`);
    }
    lines.push("");
  });

  lines.push("=== AVAILABLE EVIDENCE ===", "");

  // Similar Papers
  if (similarPapers.length > 0) {
    lines.push("SIMILAR PAPERS:", "");
    similarPapers.forEach((paper, index) => {
      lines.push(`Paper ${index + 1}: ${paper.title || "Untitled"}`);
      if (paper.authors && Array.isArray(paper.authors)) {
        lines.push(`  Authors: ${paper.authors.join(", ")}`);
      }
      if (paper.year) {
        lines.push(`  Year: ${paper.year}`);
      }
      if (paper.whyRelevant) {
        lines.push(`  Relevance: ${paper.whyRelevant}`);
      }
      if (paper.overlapHighlights && Array.isArray(paper.overlapHighlights)) {
        lines.push(`  Key Findings: ${paper.overlapHighlights.join("; ")}`);
      }
      lines.push("");
    });
  } else {
    lines.push("SIMILAR PAPERS: None available", "");
  }

  // Research Groups
  if (researchGroups.length > 0) {
    lines.push("RESEARCH GROUPS:", "");
    researchGroups.forEach((rg, index) => {
      lines.push(`Research Context ${index + 1}: ${rg.title || "Unknown paper"}`);
      if (rg.groups && Array.isArray(rg.groups)) {
        rg.groups.forEach((group) => {
          lines.push(`  Group: ${group.name || "Unnamed"}`);
          if (group.institution) {
            lines.push(`    Institution: ${group.institution}`);
          }
          if (group.notes) {
            lines.push(`    Focus: ${group.notes}`);
          }
        });
      }
      lines.push("");
    });
  } else {
    lines.push("RESEARCH GROUPS: None available", "");
  }

  // Researcher Theses
  if (researcherTheses.length > 0) {
    lines.push("PHD THESES:", "");
    researcherTheses.forEach((researcher, index) => {
      lines.push(`Researcher ${index + 1}: ${researcher.name || "Unknown"}`);
      if (researcher.phd_thesis) {
        const thesis = researcher.phd_thesis;
        lines.push(`  Thesis: ${thesis.title || "Not found"}`);
        if (thesis.year) {
          lines.push(`  Year: ${thesis.year}`);
        }
        if (thesis.institution) {
          lines.push(`  Institution: ${thesis.institution}`);
        }
      }
      if (researcher.latest_publication) {
        const pub = researcher.latest_publication;
        lines.push(`  Latest Publication: ${pub.title || "Not found"}`);
      }
      if (researcher.data_publicly_available) {
        lines.push(`  Data Available: ${researcher.data_publicly_available}`);
      }
      lines.push("");
    });
  } else {
    lines.push("PHD THESES: None available", "");
  }

  // Patents
  if (patents.length > 0) {
    lines.push("RELATED PATENTS:", "");
    patents.forEach((patent, index) => {
      lines.push(`Patent ${index + 1}: ${patent.patentNumber || "Unknown"}`);
      lines.push(`  Title: ${patent.title || "Untitled"}`);
      if (patent.assignee) {
        lines.push(`  Assignee: ${patent.assignee}`);
      }
      if (patent.overlapWithPaper && patent.overlapWithPaper.claimIds) {
        lines.push(`  Overlaps with claims: ${patent.overlapWithPaper.claimIds.join(", ")}`);
      }
      if (patent.overlapWithPaper && patent.overlapWithPaper.summary) {
        lines.push(`  Technical Overlap: ${patent.overlapWithPaper.summary}`);
      }
      lines.push("");
    });
  } else {
    lines.push("RELATED PATENTS: None available", "");
  }

  lines.push(
    "=== VERIFICATION METHODOLOGY ===",
    "",
    "CRITICAL STANCE: Be skeptical and rigorous. Assume claims are UNVERIFIED until proven otherwise.",
    "Default to 'Partially Verified' - most claims should have caveats. 'Verified' status is RARE.",
    "",
    "For each claim (C1, C2, etc.):",
    "",
    "1. INDEPENDENCE CHECK:",
    "   - Evidence from the SAME research group or authors = NOT independent validation",
    "   - Require 3+ INDEPENDENT sources (different groups/institutions) for 'Verified' status",
    "   - Same-group evidence can only support 'Partially Verified' at best",
    "",
    "2. DATA AVAILABILITY CHECK:",
    "   - Is raw data publicly available? (GitHub, Zenodo, institutional repository)",
    "   - Is code/analysis pipeline shared?",
    "   - Can findings be reproduced by an independent researcher?",
    "   - NO public data/code = automatic downgrade from 'Verified' to 'Partially Verified'",
    "",
    "3. STATISTICAL RIGOR CHECK:",
    "   - Adequate sample size (N)?",
    "   - Proper controls and randomization?",
    "   - P-values reported and appropriate?",
    "   - Effect sizes meaningful?",
    "   - Missing any of these = note as limitation",
    "",
    "4. REPLICATION CHECK:",
    "   - Has the finding been replicated by another group?",
    "   - Do similar papers CONFIRM or CONTRADICT?",
    "   - Are methods validated across multiple studies?",
    "   - No independent replication = 'Partially Verified' at best",
    "",
    "5. METHODOLOGICAL SOUNDNESS:",
    "   - Appropriate study design for the claim?",
    "   - Potential confounders addressed?",
    "   - Limitations acknowledged?",
    "   - Look for gaps in reasoning or methodology",
    "",
    "6. CONTRADICTION SEARCH (CRITICAL):",
    "   - Actively look for contradicting evidence",
    "   - Check if similar papers show different results",
    "   - Note any inconsistencies in methods or findings",
    "   - Patents showing prior art = potential contradiction",
    "   - If ANY contradictions found, cannot be 'Verified'",
    "",
    "7. VERIFICATION STATUS ASSIGNMENT (STRICT CRITERIA):",
    "",
    "   ✅ VERIFIED (RARE - only if ALL criteria met):",
    "      • 3+ independent sources confirm the claim",
    "      • NO contradicting evidence",
    "      • Data AND code publicly available",
    "      • Methods replicated by other groups",
    "      • Statistical rigor confirmed (adequate N, controls, p-values)",
    "      • No significant methodological limitations",
    "",
    "   ⚠️  PARTIALLY VERIFIED (MOST COMMON - default for reasonable claims):",
    "      • 1-2 supporting sources (may include same group)",
    "      • Minor contradictions, gaps, or limitations present",
    "      • Limited or no data availability",
    "      • Not independently replicated yet",
    "      • Some methodological concerns",
    "      • Evidence suggests claim is directionally correct but needs more validation",
    "",
    "   ❌ CONTRADICTED:",
    "      • Evidence actively refutes the claim",
    "      • Replication attempts failed",
    "      • Statistical or methodological flaws identified",
    "      • Contradicting papers outnumber supporting ones",
    "",
    "   ❓ INSUFFICIENT EVIDENCE:",
    "      • Less than 1 supporting source",
    "      • No independent validation available",
    "      • Missing key information needed to verify",
    "      • Claim is too vague to verify against available evidence",
    "",
    "8. CONFIDENCE LEVEL ASSIGNMENT:",
    "   - High: Only for 'Verified' claims with overwhelming evidence",
    "   - Moderate: For 'Partially Verified' with reasonable support",
    "   - Low: For 'Partially Verified' with minimal support or 'Insufficient Evidence'",
    "",
    "9. EVIDENCE DOCUMENTATION:",
    "   - List ALL supporting evidence with specific relevance notes",
    "   - List ALL contradicting evidence (actively search for these)",
    "   - Be specific about what each source contributes",
    "",
    "10. VERIFICATION SUMMARY:",
    "    - 2-3 sentences explaining status and reasoning",
    "    - Explicitly state limitations or caveats",
    "    - Note what additional evidence would strengthen verification",
    "",
    "IMPORTANT: Most claims should be 'Partially Verified'. If you mark everything as 'Verified', you are NOT being critical enough.",
    "",
    "=== DELIVERABLE ===",
    "",
    "For each claim, provide:",
    "- Claim ID (C1, C2, etc.)",
    "- Original Claim (verbatim)",
    "- Verification Status (Verified/Partially Verified/Contradicted/Insufficient Evidence)",
    "- Supporting Evidence:",
    "  * Source type (Similar Paper/Patent/Research Group/Thesis)",
    "  * Title/identifier",
    "  * Brief relevance note (how it supports)",
    "- Contradicting Evidence (if any):",
    "  * Source type",
    "  * Title/identifier",
    "  * Brief relevance note (how it contradicts)",
    "- Verification Summary (2-3 sentences explaining status and reasoning)",
    "- Confidence Level (High/Moderate/Low)",
    "",
    "Also provide:",
    "- Overall Assessment: Brief paragraph on the paper's overall claim validity",
    ""
  );

  return lines.join("\n");
}

function buildCleanupPrompt() {
  return [
    CLEANUP_PROMPT_HEADER.trim(),
    "",
    "Refer to the analyst notes in the previous message (do not paste them here).",
    "---",
    "[Notes already provided above]",
    "---",
    "Return the JSON object now."
  ].join("\n");
}

function normalizeEvidence(entry) {
  if (!entry || typeof entry !== "object") {
    return null;
  }

  const source = typeof entry.source === "string" ? cleanPlainText(entry.source) : "";
  const title = typeof entry.title === "string" ? cleanPlainText(entry.title) : "";
  const relevance = typeof entry.relevance === "string" ? cleanPlainText(entry.relevance) : "";

  if (!source || !title) {
    return null;
  }

  return { source, title, relevance };
}

function normalizeVerificationStatus(value) {
  if (typeof value !== "string") {
    return "Insufficient Evidence";
  }
  const normalized = value.trim();
  const valid = ["Verified", "Partially Verified", "Contradicted", "Insufficient Evidence"];
  return valid.includes(normalized) ? normalized : "Insufficient Evidence";
}

function normalizeConfidenceLevel(value) {
  if (typeof value !== "string") {
    return "Low";
  }
  const normalized = value.trim();
  const valid = ["High", "Moderate", "Low"];
  return valid.includes(normalized) ? normalized : "Low";
}

function normalizeVerifiedClaim(entry) {
  if (!entry || typeof entry !== "object") {
    return null;
  }

  const claimId = typeof entry.claimId === "string" ? entry.claimId.trim() : "";
  const originalClaim = typeof entry.originalClaim === "string" ? cleanPlainText(entry.originalClaim) : "";

  if (!claimId || !originalClaim) {
    return null;
  }

  const verificationStatus = normalizeVerificationStatus(entry.verificationStatus);
  const confidenceLevel = normalizeConfidenceLevel(entry.confidenceLevel);
  const verificationSummary = typeof entry.verificationSummary === "string"
    ? cleanPlainText(entry.verificationSummary)
    : "";

  const supportingEvidence = Array.isArray(entry.supportingEvidence)
    ? entry.supportingEvidence.map(normalizeEvidence).filter(Boolean)
    : [];

  const contradictingEvidence = Array.isArray(entry.contradictingEvidence)
    ? entry.contradictingEvidence.map(normalizeEvidence).filter(Boolean)
    : [];

  return {
    claimId,
    originalClaim,
    verificationStatus,
    supportingEvidence,
    contradictingEvidence,
    verificationSummary,
    confidenceLevel
  };
}

function normalizeVerifiedClaimsPayload(payload) {
  if (!payload || typeof payload !== "object") {
    throw new Error("Cleanup agent response must be a JSON object.");
  }

  if (!Array.isArray(payload.claims)) {
    throw new Error("claims must be an array.");
  }

  const claims = payload.claims
    .map((entry) => normalizeVerifiedClaim(entry))
    .filter(Boolean);

  if (claims.length === 0) {
    throw new Error("No valid verified claims after normalization.");
  }

  const overallAssessment = typeof payload.overallAssessment === "string"
    ? cleanPlainText(payload.overallAssessment)
    : "";

  const promptNotes = typeof payload.promptNotes === "string"
    ? cleanPlainText(payload.promptNotes)
    : "";

  return {
    claims,
    overallAssessment,
    promptNotes
  };
}

function formatVerifiedClaims(verifiedClaims) {
  const lines = [];

  if (verifiedClaims.overallAssessment) {
    lines.push("=== OVERALL ASSESSMENT ===");
    lines.push(verifiedClaims.overallAssessment);
    lines.push("");
  }

  lines.push("=== VERIFIED CLAIMS ===");
  lines.push("");

  verifiedClaims.claims.forEach((claim) => {
    lines.push(`Claim: ${claim.claimId}`);
    lines.push(`Original: ${claim.originalClaim}`);
    lines.push(`Status: ${claim.verificationStatus}`);
    lines.push(`Confidence: ${claim.confidenceLevel}`);
    lines.push("");

    if (claim.supportingEvidence.length > 0) {
      lines.push("Supporting Evidence:");
      claim.supportingEvidence.forEach((evidence) => {
        lines.push(`  - [${evidence.source}] ${evidence.title}`);
        if (evidence.relevance) {
          lines.push(`    ${evidence.relevance}`);
        }
      });
      lines.push("");
    }

    if (claim.contradictingEvidence.length > 0) {
      lines.push("Contradicting Evidence:");
      claim.contradictingEvidence.forEach((evidence) => {
        lines.push(`  - [${evidence.source}] ${evidence.title}`);
        if (evidence.relevance) {
          lines.push(`    ${evidence.relevance}`);
        }
      });
      lines.push("");
    }

    if (claim.verificationSummary) {
      lines.push(`Summary: ${claim.verificationSummary}`);
    }

    lines.push("");
  });

  return lines.join("\n");
}

async function run() {
  const rl = createInterface();
  const workingDir = process.cwd();

  try {
    console.log("\n=== Verified Claims Prompt Helper ===\n");
    console.log(`Working directory: ${workingDir}`);

    const outputPath = path.resolve(workingDir, DEFAULT_OUTPUT_PATH);
    const existingLibrary = readExistingLibrary(outputPath);

    if (!existingLibrary) {
      console.error("\n❌ No existing mock library found. Run the Similar Papers generator first.");
      return;
    }

    if (!existingLibrary.claimsAnalysis) {
      console.error("\n❌ Claims analysis data missing. Run the claims analysis generator first.");
      return;
    }

    const verificationPrompt = buildVerificationPrompt(existingLibrary);
    await clipboardy.write(verificationPrompt);

    console.log("\nVerification prompt copied to your clipboard. Paste it into your research agent to verify claims.\n");
    console.log("Preview:");
    console.log(`${verificationPrompt.slice(0, 300)}${verificationPrompt.length > 300 ? "…" : ""}`);
    console.log(
      "\nNext steps:\n  1. Paste the prompt into your research agent and let it complete.\n  2. Collect the verification notes.\n  3. Press ENTER here when you're ready for the cleanup prompt.\n"
    );

    await ask(rl, "\nPress ENTER once the notes are ready to receive the cleanup prompt: ");

    const cleanupPrompt = buildCleanupPrompt();
    await clipboardy.write(cleanupPrompt);

    console.log("\nCleanup prompt copied to your clipboard. Paste it into the cleanup agent, add the notes beneath the divider, and request JSON.\n");
    console.log("Preview:");
    console.log(`${cleanupPrompt.slice(0, 240)}${cleanupPrompt.length > 240 ? "…" : ""}`);
    console.log(
      "\nNext steps:\n  1. Paste the cleanup prompt into your LLM.\n  2. Add the verification notes beneath the placeholder line, then run the cleanup.\n  3. Paste the cleaned JSON back here (press ENTER on an empty line when finished).\n"
    );

    const cleanedJsonRaw = await collectCleanedJson(rl);

    if (!cleanedJsonRaw) {
      console.log("No cleaned JSON provided. Mock library left unchanged.");
      return;
    }

    let cleanedPayload;
    try {
      cleanedPayload = JSON.parse(cleanedJsonRaw);
    } catch (error) {
      console.error("\n❌ Failed to parse the verified claims JSON. Ensure the cleanup agent returns valid JSON only.");
      console.error("Raw snippet preview:");
      console.error(cleanedJsonRaw.slice(0, 200));
      throw new Error(`Failed to parse verified claims JSON: ${error instanceof Error ? error.message : String(error)}`);
    }

    const normalised = normalizeVerifiedClaimsPayload(cleanedPayload);
    const formattedText = formatVerifiedClaims(normalised);

    const verifiedClaimsData = {
      text: formattedText,
      structured: {
        claims: normalised.claims,
        overallAssessment: normalised.overallAssessment,
        promptNotes: normalised.promptNotes
      }
    };

    const libraryData = {
      ...existingLibrary,
      generatedAt: new Date().toISOString(),
      verifiedClaims: verifiedClaimsData
    };

    writeMockLibrary(outputPath, libraryData);

    console.log(`\nMock library updated with verified claims: ${path.relative(workingDir, DEFAULT_OUTPUT_PATH)}`);
    console.log(`\nVerified ${normalised.claims.length} claim(s).`);
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    console.error(`\n❌ ${message}`);
    process.exitCode = 1;
  } finally {
    rl.close();
  }
}

run();
